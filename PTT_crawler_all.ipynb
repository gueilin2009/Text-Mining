{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT 看板(Board) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Culture, Consumption, Politics, Gossip....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSkills：\\n1.Web Crawler\\n2.DataPipeLine\\n3.Dockefile/Docker/DockerHub(port)\\n4.DataMining(numeric, text mining, image, opinion)\\n5.Machine Learning/Deep learning\\n6.Text categorization: Topic mining, Sentiment anaylysis, Opinion mining\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Skills：\n",
    "1.Web Crawler\n",
    "2.DataPipeLine\n",
    "3.Dockefile/Docker/DockerHub(port)\n",
    "4.DataMining(numeric, text mining, image, opinion)\n",
    "5.Machine Learning/Deep learning\n",
    "6.Text categorization: Topic mining, Sentiment anaylysis, Opinion mining\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a MongoDB docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把爬取的資料部署裝到容器中, 在Linux中直接執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl -fsSL get.docker.com -o get-docker.sh | sh  #以Linux部署docker容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for gueilin2009: "
     ]
    }
   ],
   "source": [
    "!sudo docker run -p 27017:27017 -v $(pwd)/mongodb_data:/data/db/text --name some-mongo-text-mining -d mongo  \n",
    "#以port號run一台mongodb的容器,把本地端資料餵進去,輸入password後得到CONTAINER ID, NAME如下：\n",
    "#CONTAINER ID: 0b6ec25f69cc  \n",
    "#ONTAINER Names: some-mongo-text-mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in ./.pyenv/versions/3.6.5/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo  #載入pymongo連線套件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Pipeline Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#功能 (url位址即檔案)\n",
    "'''\n",
    "1.將request的網址自動寫入檔案\n",
    "2.任意設定PTT看板(Board)分類與爬取筆數(Num),每個板約3000多個網頁/每頁約10-20筆文章\n",
    "3.自動化擷取各檔案的頁面資訊,餵到雲端容器中\n",
    "4.創BOW(Bag of Words)詞向量,依字頻排序（ex：英文部分）\n",
    ".....\n",
    ".....\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "import re\n",
    "!pip install BeautifulSoup4  #Linux packages: BeautifulSoup4\n",
    "from bs4 import BeautifulSoup   \n",
    "from collections import Counter \n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "#取出感興趣的看板中的網頁\n",
    "def get_ptt(Board, Num):  \n",
    "    main_page = 'https://www.ptt.cc/bbs/%s'%Board + '/index.html'  \n",
    "    response = requests.get(main_page)\n",
    "    total_page = int(response.text.split('a class=\"btn wide\" href=\"/bbs/%s'%Board +'/index')[2].split('.html\">&lsaquo; 上頁</a>')[0]) + 1\n",
    "    url_template = 'https://www.ptt.cc/bbs/%s'%Board + '/index%s.html'    \n",
    "    pages_to_crawl = Num  \n",
    "    for page in range(total_page, total_page - pages_to_crawl,-1):  \n",
    "        urls = url_template%page  \n",
    "        return urls\n",
    "    \n",
    "    \n",
    "#取出每個網頁中每筆ptt文章的urls存檔    \n",
    "def get_ptt_detail(urls, Board, Num):   \n",
    "    with open('./ptt_data_all/%s'%Num, 'w') as f:   #Linux mkdir ptt_data_all\n",
    "        html=f.write(requests.get(urls, headers={\"cookie\":\"over18=1\"}).text)\n",
    "    with open('./ptt_data_all/%s'%Num) as f: \n",
    "        html=f.read()\n",
    "        detail_urls_list = re.findall('/bbs/%s'%Board + '/M.{18}html', html)\n",
    "        return detail_urls_list  \n",
    "\n",
    "\n",
    "#parse每一則ptt文章的資訊\n",
    "def parse_ptt_detail(detail_urls_list, Num_detail):\n",
    "    for i in detail_urls_list:\n",
    "        with open('./ptt_data_all/details/%s'%Num_detail, 'w') as f:   #linux可直接touch files\n",
    "            html = f.write(requests.get('https://www.ptt.cc/%s'%i).text)\n",
    "        with open('./ptt_data_all/details/%s'%Num_detail) as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser') #Linux using html.parser  Wins using lxml\n",
    "            metas = [tag.text for tag in soup.find_all('span', {\"class\":\"article-meta-value\"})] \n",
    "            ptt_data = {}\n",
    "            ptt_data['author']  = metas[0]\n",
    "            ptt_data['board'] = metas[1]\n",
    "            ptt_data['title'] = metas[2]\n",
    "            ptt_data['dt'] = metas[3]\n",
    "            content = [tag.text for tag in soup.find_all('div', {\"id\":\"main-content\"}, {\"class\":\"bbs-screen bbs-content\"})] \n",
    "            ptt_data['content'] = content  #推文可再以div.push過濾出來\n",
    "            return ptt_data\n",
    "\n",
    "        \n",
    "#把資料存到雲端容器, 所有code可直接寫成docker file(shell script)\n",
    "def save_to_mongo(ptt_data):   \n",
    "    client = MongoClient()\n",
    "    client.crawler.ptt.insert([ptt_data])  #crawler: database, ptt: collection   #取出塞入的文章id:\n",
    "    return client\n",
    "\n",
    "\n",
    "#進入容器, 取出文章text後, 創BOW,或利用R斷詞套件(合併寫在docker file中)\n",
    "def cnt_words(text, res={})  #arg直接創dict\n",
    "    bag = text.replace(',','').replace('.','').replace('?','').lower().split()\n",
    "    for word in bag:\n",
    "        if word not in stopwords:  #設定stopwords為另一篇ptt文\n",
    "            if word in res:\n",
    "                res[word] += 1    #統計wordf freq\n",
    "            else:\n",
    "                res[word] = 1    #第一次進來會在這裡\n",
    "            return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#假設設定要爬NBA板10頁\n",
    "\n",
    "urls = get_ptt('NBA', 10)\n",
    "detail_urls_list = get_ptt_detail(urls, 'NBA', 10)\n",
    "ptt_data = parse_ptt_detail(detail_urls_list, 15)  #已爬取\n",
    "client = save_to_mongo(ptt_data)  \n",
    "#res = cnt_words(text, res={})   \n",
    "#res.most_common()   #word freq排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client  #container key mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), u'crawler')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = client.crawler\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db.ptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), u'crawler'), u'ptt')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#進入空器中處理文章,全部完成後打成images上傳dockerhub\n",
    "# sudo docker exec -it 0b6ec25f69cc  /bin/bash\n",
    "# root@0b6ec25f69cc:/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
